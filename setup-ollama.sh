#!/bin/bash

# Script de configuration Ollama avec mod√®le
# Nuit de l'Info 2025

set -e

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

cd "$(dirname "$0")"

echo -e "${BLUE}ü§ñ Configuration Ollama - Nuit de l'Info 2025${NC}"
echo "=============================================="
echo ""

# V√©rifier si Ollama tourne
echo -e "${YELLOW}[1/5]${NC} V√©rification du service Ollama..."
if ! sudo docker ps | grep -q nuitinfo_ollama; then
    echo "Ollama n'est pas d√©marr√©. Lancement..."
    sudo docker-compose up -d ollama
    echo "Attente du d√©marrage d'Ollama (30 secondes)..."
    sleep 30
else
    echo -e "${GREEN}‚úì${NC} Ollama est d√©j√† d√©marr√©"
fi

echo ""
echo -e "${YELLOW}[2/5]${NC} Choix du mod√®le √† t√©l√©charger"
echo ""
echo "Mod√®les recommand√©s pour un chatbot :"
echo "  1) llama3.2:3b     - Petit, rapide (2 GB)"
echo "  2) phi3:mini       - Tr√®s l√©ger (2.3 GB)"
echo "  3) mistral:7b      - Bon √©quilibre (4.1 GB)"
echo "  4) llama3.2:1b     - Ultra-l√©ger (1.3 GB) - Recommand√© pour d√©marrer"
echo "  5) Autre mod√®le"
echo ""
echo -n "Votre choix [1-5] (d√©faut: 4): "
read CHOICE

case ${CHOICE:-4} in
    1)
        MODEL="llama3.2:3b"
        ;;
    2)
        MODEL="phi3:mini"
        ;;
    3)
        MODEL="mistral:7b"
        ;;
    4)
        MODEL="llama3.2:1b"
        ;;
    5)
        echo -n "Nom du mod√®le : "
        read MODEL
        ;;
    *)
        MODEL="llama3.2:1b"
        ;;
esac

echo ""
echo -e "${YELLOW}[3/5]${NC} T√©l√©chargement du mod√®le ${MODEL}..."
echo "Cela peut prendre quelques minutes selon votre connexion..."
sudo docker exec nuitinfo_ollama ollama pull ${MODEL}

echo ""
echo -e "${YELLOW}[4/5]${NC} Test du mod√®le..."
echo "Question test : 'Bonjour, qui es-tu ?'"
echo ""
RESPONSE=$(sudo docker exec nuitinfo_ollama ollama run ${MODEL} "Bonjour, qui es-tu ? R√©ponds en une phrase courte.")
echo -e "${GREEN}R√©ponse du mod√®le :${NC}"
echo "$RESPONSE"

echo ""
echo -e "${YELLOW}[5/5]${NC} D√©marrage de l'interface Web..."
sudo docker-compose up -d ollama-webui
sleep 5

echo ""
echo -e "${GREEN}üéâ Configuration termin√©e !${NC}"
echo ""
echo "üìù Acc√®s au chatbot :"
echo "  Interface Web : http://localhost:3000"
echo "  API Ollama    : http://localhost:11434"
echo ""
echo "üîß Utilisation de l'API :"
echo ""
echo "# Lister les mod√®les install√©s"
echo "curl http://localhost:11434/api/tags"
echo ""
echo "# Envoyer un message au chatbot"
echo "curl http://localhost:11434/api/generate -d '{"
echo "  \"model\": \"${MODEL}\","
echo "  \"prompt\": \"Explique ce qu'est un oc√©an en une phrase\","
echo "  \"stream\": false"
echo "}'"
echo ""
echo "üìö Mod√®le actif : ${MODEL}"
echo ""
echo "üí° Pour tester depuis Python :"
echo ""
cat << 'PYTHON_EOF'
import requests

def chat_with_ollama(message, model="llama3.2:1b"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": message,
            "stream": False
        }
    )
    return response.json()["response"]

# Test
print(chat_with_ollama("Bonjour !"))
PYTHON_EOF

echo ""
echo "üåê Interface Web Ollama :"
echo "  Ouvrez http://localhost:3000 dans votre navigateur"
echo "  Cr√©ez un compte (local, stock√© dans le container)"
echo "  Commencez √† chatter avec le mod√®le ${MODEL}"
echo ""
