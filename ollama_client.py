#!/usr/bin/env python3
"""
Client Python pour interagir avec l'API Ollama
Nuit de l'Info 2025
"""

import requests
import json
import sys


class OllamaClient:
    """Client pour l'API Ollama"""

    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url

    def list_models(self):
        """Liste tous les mod√®les install√©s"""
        response = requests.get(f"{self.base_url}/api/tags")
        return response.json()

    def generate(self, model, prompt, stream=False):
        """
        G√©n√®re une r√©ponse du mod√®le

        Args:
            model: Nom du mod√®le (ex: "llama3.2:1b")
            prompt: Question/prompt √† envoyer
            stream: Si True, retourne un g√©n√©rateur pour streaming
        """
        if stream:
            return self._generate_stream(model, prompt)
        else:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False
            }
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data
            )
            return response.json()

    def _generate_stream(self, model, prompt):
        """G√©n√©rateur pour le streaming"""
        data = {
            "model": model,
            "prompt": prompt,
            "stream": True
        }
        response = requests.post(
            f"{self.base_url}/api/generate",
            json=data,
            stream=True
        )
        for line in response.iter_lines():
            if line:
                yield json.loads(line)

    def chat(self, model, messages, stream=False):
        """
        Chat avec historique de conversation

        Args:
            model: Nom du mod√®le
            messages: Liste de messages [{"role": "user", "content": "..."}]
            stream: Si True, retourne un g√©n√©rateur pour streaming
        """
        if stream:
            return self._chat_stream(model, messages)
        else:
            data = {
                "model": model,
                "messages": messages,
                "stream": False
            }
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=data
            )
            return response.json()

    def _chat_stream(self, model, messages):
        """G√©n√©rateur pour le streaming du chat"""
        data = {
            "model": model,
            "messages": messages,
            "stream": True
        }
        response = requests.post(
            f"{self.base_url}/api/chat",
            json=data,
            stream=True
        )
        for line in response.iter_lines():
            if line:
                yield json.loads(line)


def main():
    """Exemples d'utilisation"""
    client = OllamaClient()

    print("=" * 60)
    print("CLIENT API OLLAMA - Nuit de l'Info 2025")
    print("=" * 60)
    print()

    # 1. Lister les mod√®les disponibles
    print("üìã Mod√®les install√©s:")
    print("-" * 60)
    try:
        models = client.list_models()
        if "models" in models and models["models"]:
            for model in models["models"]:
                print(f"  - {model['name']}")
                print(f"    Taille: {model.get('size', 'N/A')} bytes")
                print(f"    Modifi√©: {model.get('modified_at', 'N/A')}")
                print()
        else:
            print("  ‚ö†Ô∏è  Aucun mod√®le install√©")
            print("  Utilisez: docker exec nuitinfo_ollama ollama pull llama3.2:1b")
            return
    except requests.exceptions.ConnectionError:
        print("  ‚ùå Erreur: Impossible de se connecter √† Ollama")
        print("  V√©rifiez que le service est d√©marr√©: sudo docker-compose up -d ollama")
        return

    # S√©lectionner le premier mod√®le disponible
    model_name = models["models"][0]["name"]
    print()
    print(f"ü§ñ Utilisation du mod√®le: {model_name}")
    print("=" * 60)
    print()

    # 2. Exemple de g√©n√©ration simple (sans streaming)
    print("üí¨ Exemple 1: G√©n√©ration simple")
    print("-" * 60)
    prompt = "Explique en une phrase ce qu'est un oc√©an."
    print(f"Prompt: {prompt}")
    print()

    response = client.generate(model_name, prompt, stream=False)
    print(f"R√©ponse: {response['response']}")
    print()

    # 3. Exemple de chat avec historique
    print()
    print("üí¨ Exemple 2: Chat avec historique")
    print("-" * 60)
    messages = [
        {"role": "user", "content": "Bonjour! Peux-tu me parler des oc√©ans?"},
    ]

    response = client.chat(model_name, messages, stream=False)
    assistant_message = response["message"]["content"]
    print(f"User: {messages[0]['content']}")
    print(f"Assistant: {assistant_message}")
    print()

    # Continuer la conversation
    messages.append({"role": "assistant", "content": assistant_message})
    messages.append({"role": "user", "content": "Pourquoi sont-ils importants pour la plan√®te?"})

    response = client.chat(model_name, messages, stream=False)
    print(f"User: {messages[-1]['content']}")
    print(f"Assistant: {response['message']['content']}")
    print()

    # 4. Exemple de streaming
    print()
    print("üí¨ Exemple 3: Streaming (r√©ponse en temps r√©el)")
    print("-" * 60)
    prompt = "Cite 3 menaces pour les oc√©ans."
    print(f"Prompt: {prompt}")
    print("R√©ponse (streaming): ", end="", flush=True)

    full_response = ""
    for chunk in client.generate(model_name, prompt, stream=True):
        if "response" in chunk:
            token = chunk["response"]
            full_response += token
            print(token, end="", flush=True)

    print("\n")
    print("=" * 60)
    print()


if __name__ == "__main__":
    main()
